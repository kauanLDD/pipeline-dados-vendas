{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Bronze Layer - Ingestao de Dados\n",
        "\n",
        "Este notebook realiza a extracao de dados brutos e adiciona metadados de ingestao.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import current_timestamp, lit\n",
        "import urllib.request\n",
        "import os\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Bronze Layer\") \\\n",
        "    .getOrCreate()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Baixando dados do GitHub e criando diretorios\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "url_github = \"https://raw.githubusercontent.com/kauanLDD/pipeline-dados-vendas/main/data.csv\"\n",
        "arquivo_local = \"data/source/data.csv\"\n",
        "\n",
        "os.makedirs(\"data/source\", exist_ok=True)\n",
        "os.makedirs(\"data/bronze\", exist_ok=True)\n",
        "\n",
        "urllib.request.urlretrieve(url_github, arquivo_local)\n",
        "\n",
        "df_raw = spark.read.csv(arquivo_local, header=True, inferSchema=True)\n",
        "\n",
        "print(f\"Dados lidos: {df_raw.count()} linhas\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Adicionando metadados e salvando em Parquet\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_with_metadata = df_raw \\\n",
        "    .withColumn(\"data_ingestao\", current_timestamp()) \\\n",
        "    .withColumn(\"fonte_arquivo\", lit(\"github/pipeline-dados-vendas/data.csv\"))\n",
        "\n",
        "output_path = \"data/bronze/dados_brutos.parquet\"\n",
        "\n",
        "df_with_metadata.write \\\n",
        "    .mode(\"overwrite\") \\\n",
        "    .parquet(output_path)\n",
        "\n",
        "print(f\"Dados salvos em: {output_path}\")\n",
        "\n",
        "df_verificacao = spark.read.parquet(output_path)\n",
        "print(f\"Verificacao: {df_verificacao.count()} linhas salvas\")\n",
        "\n",
        "spark.stop()\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
